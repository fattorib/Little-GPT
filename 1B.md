
| Model        | LAMBADA (Acc) | LAMBADA (PPL) |   WikiText (PPL)  | Piqa (Acc) | Hellaswag (Acc) | Winogrande (Acc) | Training Tokens |
|--------------|:-------------:|:-------------:|:-----------------:|:----------:|:---------------:|:----------------:|:---------------:|
| **GPT-1B\*** | 52.65         | 9.758         | 23.052 (1024 CTX) | 69.31%     | 33.36%          | 52.17%           | 26B             |
| GPT-2 1.5B   | 51.21%        | 10.634        | 17.48 (1024 CTX)  | 70.78%     | 40.03%          | 59.40%           | -               |
| GPT-Neo 1.3B | 57.23         | 7.498         | 13.10 (2048 CTX)  | 71.11%     | 38.66           | 55.01            | 300B            |


| Model               | enwik8 (BPB) | text8 (BPC) | WikiText (PPL) |
|---------------------|--------------|:-----------:|:--------------:|
| GPT-1B\* (1024 ctx) | 1.134        | 1.174       | 23.052         |
| GPT-1B\* (2048 ctx) | 1.141        | 1.182       | 23.318         |


